{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Z_vsLu3XA0NO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os \n",
        "path =\"/kaggle/input/movie-review-polarity/txt_sentoken/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3amS9jizBIt9",
        "outputId": "8d594291-8595-4400-ff24-03bd89e150d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n",
            "Dataset downloaded successfully.\n",
            "Extracting dataset...\n",
            "Dataset extracted successfully.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os \n",
        "from nltk.corpus import stopwords \n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "\n",
        "import urllib.request\n",
        "import tarfile\n",
        "# Define the URL of the dataset\n",
        "url = \"http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\"\n",
        "\n",
        "# Define the destination folder to save the dataset\n",
        "destination_folder = \"./review_polarity\"\n",
        "\n",
        "# Download the dataset\n",
        "print(\"Downloading dataset...\")\n",
        "urllib.request.urlretrieve(url, \"review_polarity.tar.gz\")\n",
        "print(\"Dataset downloaded successfully.\")\n",
        "\n",
        "# Extract the dataset\n",
        "print(\"Extracting dataset...\")\n",
        "with tarfile.open(\"review_polarity.tar.gz\", \"r:gz\") as tar:\n",
        "    tar.extractall(destination_folder)\n",
        "print(\"Dataset extracted successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1qYM97uHDq4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21b8de6e-f3fb-41ef-c7e3-de0a56832087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ykbluLGABANu"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords \n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.models import load_model\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNx3MuO5CKn_",
        "outputId": "7ce3a789-e7dc-483a-e812-861590bf1750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords and wordnet\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "q_ZMBN8nBQoo"
      },
      "outputs": [],
      "source": [
        "stop_words = stopwords.words('english')\n",
        "# i have to make a document and label s\n",
        "\n",
        "def load_doc(filename):\n",
        "    file = open(filename,'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "\n",
        "    \n",
        "def clean_doc(doc):\n",
        "    \n",
        "    table = str.maketrans(dict.fromkeys(string.punctuation))\n",
        "    doc =  doc.translate(table)\n",
        "    doc = doc.split()\n",
        "    token = [word for word in doc   if word.isalpha()]\n",
        "    token = [word for word in token if word not in stop_words] \n",
        "    token = [word for word in token if len(word)>1]\n",
        "    \n",
        "    return token "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5ZDGTNcSBV0R"
      },
      "outputs": [],
      "source": [
        "min_occurence =2\n",
        "def process_doc(directory,vocab):\n",
        "    for filename in os.listdir(directory):\n",
        "        \n",
        "        doc =  load_doc(os.path.join(directory,filename))\n",
        "        token = clean_doc(doc)\n",
        "        vocab.update(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uSYk8GlOBYXN"
      },
      "outputs": [],
      "source": [
        "path = './review_polarity/txt_sentoken/'\n",
        "\n",
        "vocab = Counter()\n",
        "\n",
        "pos_tokens = process_doc(path+'pos',vocab)\n",
        "neg_tokens = process_doc(path+'neg',vocab)\n",
        "\n",
        "vocab.most_common(10)\n",
        "tokens = [k for k,v in vocab.items() if v>=min_occurence]\n",
        "\n",
        "def save_list(lines,filename):\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename,'w')\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "save_list(tokens,'vocab.txt')\n",
        "\n",
        "vocab = load_doc('vocab.txt')\n",
        "vocab = set(vocab.split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaQtJZ-6DIIM"
      },
      "source": [
        "# 新段落"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8lNnXZi0BfMc"
      },
      "outputs": [],
      "source": [
        "def updated_clean_doc(doc,vocab):\n",
        "    \n",
        "    table = str.maketrans(dict.fromkeys(string.punctuation))\n",
        "    doc =  doc.translate(table)\n",
        "    doc = doc.split()\n",
        "    token = [word for word in doc   if word.isalpha()]\n",
        "    token = [word for word in token if word not in stop_words] \n",
        "    token = [word for word in token if len(word)>1]\n",
        "    token = [word for word in token if word in vocab]\n",
        "    return token \n",
        "def updated_process_doc(directory,vocab):\n",
        "    documents = list()\n",
        "    for filename in os.listdir(directory):\n",
        "        \n",
        "        doc =  load_doc(os.path.join(directory,filename))\n",
        "        token = updated_clean_doc(doc,vocab)\n",
        "        lines = ' '.join(token)\n",
        "        documents.append(lines)\n",
        "    return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "n4kNgBmKBiJ6"
      },
      "outputs": [],
      "source": [
        "path = './review_polarity/txt_sentoken/'\n",
        "pos =  updated_process_doc(path+'pos',vocab)\n",
        "neg =  updated_process_doc(path+'neg',vocab)\n",
        "document = pos+ neg \n",
        "labels = [1]* len(pos ) + [0]*len(neg)\n",
        "\n",
        "assert len(labels) == len(pos)+len(pos)\n",
        "def create_tokenizer(doc):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(doc)\n",
        "    return tokenizer\n",
        "def create_encoding(doc,tokenizer,length):\n",
        "    tokens = tokenizer.texts_to_sequences(doc)\n",
        "    padded = pad_sequences(tokens,length,padding='post')\n",
        "    return padded\n",
        "max_length = max([len(i.split()) for i in document])\n",
        "max_length\n",
        "\n",
        "tokenizer = create_tokenizer(document)\n",
        "padded = create_encoding(document,tokenizer,max_length)\n",
        "\n",
        "padded = np.array(padded)\n",
        "labels = np.array(labels)\n",
        "vocab_size = len(tokenizer.word_index) +1"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mBYRTZr7NmTX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOxpaOcuBrgV",
        "outputId": "4b831015-fc63-4ddd-bea4-0c470a17507e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 1319, 100)         2714000   \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 1312, 32)          25632     \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPooling  (None, 656, 32)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 20992)             0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                209930    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,949,573\n",
            "Trainable params: 2,949,573\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "44/44 [==============================] - 18s 386ms/step - loss: 0.5887 - accuracy: 0.7136 - val_loss: 1.1104 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 18s 416ms/step - loss: 0.3792 - accuracy: 0.7607 - val_loss: 1.0204 - val_accuracy: 0.4017\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 17s 383ms/step - loss: 0.0550 - accuracy: 0.9921 - val_loss: 1.8532 - val_accuracy: 0.3667\n",
            "Epoch 4/10\n",
            "44/44 [==============================] - 17s 397ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.6537 - val_accuracy: 0.5033\n",
            "Epoch 5/10\n",
            "44/44 [==============================] - 17s 384ms/step - loss: 7.9023e-04 - accuracy: 1.0000 - val_loss: 1.8760 - val_accuracy: 0.4767\n",
            "Epoch 6/10\n",
            "44/44 [==============================] - 17s 382ms/step - loss: 3.7861e-04 - accuracy: 1.0000 - val_loss: 1.8649 - val_accuracy: 0.4917\n",
            "Epoch 7/10\n",
            "44/44 [==============================] - 17s 380ms/step - loss: 2.6325e-04 - accuracy: 1.0000 - val_loss: 1.9671 - val_accuracy: 0.4767\n",
            "Epoch 8/10\n",
            "44/44 [==============================] - 22s 497ms/step - loss: 2.0026e-04 - accuracy: 1.0000 - val_loss: 1.9459 - val_accuracy: 0.4933\n",
            "Epoch 9/10\n",
            "44/44 [==============================] - 18s 416ms/step - loss: 1.5729e-04 - accuracy: 1.0000 - val_loss: 1.9103 - val_accuracy: 0.5033\n",
            "Epoch 10/10\n",
            "44/44 [==============================] - 18s 409ms/step - loss: 1.2746e-04 - accuracy: 1.0000 - val_loss: 1.9827 - val_accuracy: 0.4983\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb62ed94d00>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "def define_model(vocab_size,max_length):\n",
        "\n",
        "    model = Sequential() \n",
        "    model.add(Embedding(vocab_size,100,input_length=max_length))\n",
        "\n",
        "    model.add(Conv1D(32,kernel_size=8,activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(10,activation='relu'))\n",
        "\n",
        "    model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "    model.summary() \n",
        "    plot_model(model,to_file='model.png',show_shapes=True)\n",
        "    return model\n",
        "os.environ[\"KMP_SETTINGS\"] = \"false\"\n",
        "model = define_model(vocab_size,max_length)\n",
        "\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model.fit(padded,labels,epochs=10,validation_split=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yk6wpiC8N2HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCh1iyg9ByNR",
        "outputId": "b2c13edc-cb6d-42a3-c276-9891250ee2ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 134ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9996484140865505, 'NEGATIVE')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "model.save('model.h5')\n",
        "def predict_sentiment(review,vocab,tokenizer,max_length,model):\n",
        "    \n",
        "    line = updated_clean_doc(review,vocab)\n",
        "    padded = create_encoding([line],tokenizer,max_length)\n",
        "    \n",
        "    yhat = model.predict(padded)\n",
        "    percent_pos = yhat[0,0]\n",
        "    if round(percent_pos) == 0:\n",
        "        return (1-percent_pos), 'NEGATIVE' \n",
        "    return percent_pos, 'POSITIVE'\n",
        "test_file = './review_polarity/txt_sentoken/neg/cv001_19502.txt'\n",
        "\n",
        "test_doc = load_doc(test_file)\n",
        "loaded_model = load_model('model.h5')\n",
        "predict_sentiment(test_doc,vocab,tokenizer,max_length,loaded_model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the maximum vocabulary size to 10,000\n",
        "max_vocab_size = 10000\n",
        "\n",
        "# Create a new tokenizer with the reduced vocabulary size\n",
        "tokenizer_trunc = Tokenizer(num_words=max_vocab_size)\n",
        "tokenizer_trunc.fit_on_texts(document)\n",
        "\n",
        "# Encode the sequences using the reduced vocabulary\n",
        "padded_trunc_vocab = create_encoding(document, tokenizer_trunc, max_length)\n"
      ],
      "metadata": {
        "id": "1hC8CfI_N5PI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_model(vocab_size,max_length):\n",
        "\n",
        "    model = Sequential() \n",
        "    model.add(Embedding(vocab_size,100,input_length=max_length))\n",
        "\n",
        "    model.add(Conv1D(32,kernel_size=8,activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(10,activation='relu'))\n",
        "\n",
        "    model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "    model.summary() \n",
        "    plot_model(model,to_file='model.png',show_shapes=True)\n",
        "    return model\n",
        "os.environ[\"KMP_SETTINGS\"] = \"false\"\n",
        "model = define_model(vocab_size,max_length)\n",
        "\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model.fit(padded,labels,epochs=10,validation_split=0.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI1YSoOCPArS",
        "outputId": "0702fba4-363b-41a4-cff4-5300cbc8af52"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 1319, 100)         2714000   \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 1312, 32)          25632     \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPooling  (None, 656, 32)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 20992)             0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 10)                209930    \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,949,573\n",
            "Trainable params: 2,949,573\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "44/44 [==============================] - 21s 429ms/step - loss: 0.6086 - accuracy: 0.7014 - val_loss: 1.0352 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 16s 372ms/step - loss: 0.4612 - accuracy: 0.7143 - val_loss: 1.3742 - val_accuracy: 0.0017\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 17s 381ms/step - loss: 0.2641 - accuracy: 0.8407 - val_loss: 1.8505 - val_accuracy: 0.0600\n",
            "Epoch 4/10\n",
            "44/44 [==============================] - 17s 380ms/step - loss: 0.1947 - accuracy: 0.9921 - val_loss: 2.6077 - val_accuracy: 0.0400\n",
            "Epoch 5/10\n",
            "44/44 [==============================] - 17s 381ms/step - loss: 0.1831 - accuracy: 0.9964 - val_loss: 2.3885 - val_accuracy: 0.1150\n",
            "Epoch 6/10\n",
            "44/44 [==============================] - 19s 436ms/step - loss: 0.1760 - accuracy: 0.9993 - val_loss: 2.7108 - val_accuracy: 0.1000\n",
            "Epoch 7/10\n",
            "44/44 [==============================] - 16s 374ms/step - loss: 0.1695 - accuracy: 0.9993 - val_loss: 2.5729 - val_accuracy: 0.1600\n",
            "Epoch 8/10\n",
            "44/44 [==============================] - 18s 409ms/step - loss: 0.1639 - accuracy: 1.0000 - val_loss: 3.0638 - val_accuracy: 0.0967\n",
            "Epoch 9/10\n",
            "44/44 [==============================] - 18s 405ms/step - loss: 0.1584 - accuracy: 1.0000 - val_loss: 3.0307 - val_accuracy: 0.1183\n",
            "Epoch 10/10\n",
            "44/44 [==============================] - 17s 378ms/step - loss: 0.1533 - accuracy: 1.0000 - val_loss: 3.2734 - val_accuracy: 0.1050\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb624ec9910>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the maximum vocabulary size to 10,000\n",
        "max_vocab_size = 10000\n",
        "\n",
        "# Create a new tokenizer with the reduced vocabulary size\n",
        "tokenizer_trunc = Tokenizer(num_words=max_vocab_size)\n",
        "tokenizer_trunc.fit_on_texts(document)\n",
        "\n",
        "# Encode the sequences using the reduced vocabulary\n",
        "padded_trunc_vocab = create_encoding(document, tokenizer_trunc, max_length)\n",
        "\n",
        "# Train a model using the reduced vocabulary\n",
        "model_trunc_vocab = ...\n",
        "model_trunc_vocab.fit(padded_trunc_vocab, labels, ...)\n"
      ],
      "metadata": {
        "id": "FL2RMrYVOLzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "# Load the pre-trained GloVe embeddings\n",
        "glove_dir = '/Users/watson/Desktop/algonquin/nlp/glove.6B/'\n",
        "embedding_dim = 100\n",
        "embeddings_index = {}\n",
        "with open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "# Load and preprocess the IMDB dataset\n",
        "max_features = 10000\n",
        "maxlen = 200\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# Prepare the embedding matrix\n",
        "embedding_matrix = np.zeros((max_features, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=False))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n"
      ],
      "metadata": {
        "id": "jmmO2AprOjKp"
      },
      "execution_count": 22,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}